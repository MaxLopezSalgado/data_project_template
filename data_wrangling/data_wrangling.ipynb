{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author: Your Name\n",
    "- First Commit: yyyy-mm-dd                      #folowing ISO  8601 Format\n",
    "- Last Commit: yyyy-mm-dd                       #folowing ISO  8601 Format\n",
    "- Description: This notebook is used to perform EDA on the \"xxxxx\" dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Steps in this Data Analysis:\n",
    "\n",
    "1. **Framing the Question:** \n",
    "   - The first step towards any sort of data analysis is to ask the right question(s) from the given data. \n",
    "   - Identifying the objective of the analysis makes it easier to decide on the type(s) of data needed to draw conclusions.\n",
    "\n",
    "2. **Data Wrangling:** \n",
    "   - Data wrangling, sometimes referred to as data munging or data pre-processing, is the process of gathering, assessing, and cleaning \"raw\" data into a form suitable for analysis.\n",
    "\n",
    "3. **Exploratory Data Analysis (EDA):** \n",
    "   - Once the data is collected, cleaned, and processed, it is ready for analysis. \n",
    "   - During this phase, you can use data analysis tools and software to understand, interpret, and derive conclusions based on the requirements.\n",
    "\n",
    "4. **Drawing Conclusions:** \n",
    "   - After completing the analysis phase, the next step is to interpret the analysis and draw conclusions. \n",
    "   - Three key questions to ask at this stage:\n",
    "     - Did the analysis answer my original question?\n",
    "     - Were there any limitations in my analysis that could affect my conclusions?\n",
    "     - Was the analysis sufficient to support decision-making?\n",
    "\n",
    "5. **Communicating Results:** \n",
    "   - Once data has been explored and conclusions have been drawn, it's time to communicate the findings to the relevant audience. \n",
    "   - Effective communication can be achieved through data storytelling, writing blogs, making presentations, or filing reports.\n",
    "\n",
    "**Note:** The five steps of data analysis are not always followed linearly. The process can be iterative, with steps revisited based on new insights or requirements that arise during the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Wrangling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Gathering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv files \n",
    "df = pd.read_csv('../datasets/df.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Assessing of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look of the data´s shape\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look of the data´s info\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look of the data´s head\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for NULL values\n",
    "display(customers.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the type of information that every column has\n",
    "display(df.dtypes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1  Remove irrelevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant data in the 'x' column. (i.e: \"In this case, we will drop the colums that have more null values than valid values\")\n",
    "df = df.drop(columns=['column1', 'column2'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2  Remove/replace null values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the columns with missing values \n",
    "# Check datatype of the columns\n",
    "display(df['column1'].dtype)  \n",
    "\n",
    "# Transform the columns into datetime type\n",
    "df['column1'] = pd.to_datetime(df['column1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of this columns\n",
    "mean_df = df['column1'].mean()\n",
    "display(mean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imput the corresponding values to the null values in this columns with the mean (or the best method for each case)\n",
    "# check if null values exist in order_data dataset\n",
    "df.isnull().sum()['column1'].fillna(mean_df, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if null values exist in order_data dataset\n",
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2.2 Dataset's columns types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the columns with missing values \n",
    "# Check datatype of the columns (they need to be in this case datime type)\n",
    "display(df['column1'].dtype)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of this columns\n",
    "mean_column1 = df['column1'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imput the corresponding values to the null values in this columns with the mean\n",
    "df['column1'].fillna(mean_column1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again if null values exist in order_data dataset\n",
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Drop the duplicates, if any"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use duplicate() function to find duplicated data in the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find duplicates based on all columns\n",
    "display(df[df.duplicated()].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates in df \n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 Type conversion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure numbers are stored as numerical data types. A date should be stored as a date object, or a Unix timestamp (number of seconds, and so on).\n",
    "In this case we already did this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5 Syntax Errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.6 Outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are values that are significantly different from all other observations. Any data value that lies more than (1.5 * IQR) away from the Q1 and Q3 quartiles is considered an outlier.\n",
    "\n",
    "In general, an e-commerce dataset obtained from a well-functioning system is less likely to have outliers compared to datasets that involve manual data entry or measurement errors. E-commerce datasets typically capture transactional information, such as customer details, product information, and order-related data, which are less prone to outliers.\n",
    "\n",
    "However, it's still possible to have outliers in certain scenarios, such as:\n",
    "\n",
    "Data entry errors: Although automated systems minimize data entry errors, there can still be instances where incorrect or extreme values are recorded.\n",
    "\n",
    "Measurement errors: If the dataset includes measurements or quantitative data collected manually, there may be measurement errors leading to outliers.\n",
    "\n",
    "System glitches or anomalies: While rare, system glitches or anomalies can occasionally result in outliers in the data.\n",
    "\n",
    "Fraudulent activities: In some cases, fraudulent transactions or activities may introduce outliers into the dataset.\n",
    "\n",
    "Therefore, while it's reasonable to assume that the occurrence of outliers in an e-commerce dataset is relatively low, it's still advisable to examine the data and apply outlier detection techniques to ensure data quality and integrity.\n",
    "\n",
    "Remember that outlier detection is an iterative process, and there is no one-size-fits-all approach. It requires a combination of domain knowledge, data understanding, and experimentation to determine the most suitable method and threshold for your specific dataset and analysis objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to find otliers\n",
    "dataframes = [df, df1, df2, df3,\n",
    "              df4]\n",
    "\n",
    "for df in dataframes:\n",
    "    # Identify numerical columns\n",
    "    numerical_columns = df.select_dtypes(include=np.number).columns\n",
    "\n",
    "    # Define percentiles for outlier detection (e.g., values outside [5th percentile, 95th percentile])\n",
    "    lower_percentile = 5\n",
    "    upper_percentile = 95\n",
    "\n",
    "    for column in numerical_columns:\n",
    "        # Calculate percentiles for the column\n",
    "        lower_threshold = np.percentile(df[column], lower_percentile)\n",
    "        upper_threshold = np.percentile(df[column], upper_percentile)\n",
    "\n",
    "        # Find rows with outliers in the column\n",
    "        outlier_rows = (df[column] < lower_threshold) | (df[column] > upper_threshold)\n",
    "\n",
    "        # Print rows with outliers in the column\n",
    "        print(df[outlier_rows])\n",
    "        print('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing outlier analysis we indentified that because of the nature of the info, there are not relevant outliers, althought numerically there are some that exists. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.7 In-record & cross-datasets errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These errors result from having two or more values in the same row or across datasets that contradict with each other. For example, if we have a dataset about the cost of living in cities. The total column must be equivalent to the sum of rent, transport, and food."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to export the cleaned data to perform EDA in the future\n",
    "# Specify the path to the dataset folder\n",
    "folder_path = \"../datasets/\"\n",
    "\n",
    "dataframes = [df, df1, df2, df3,\n",
    "              df4]\n",
    "\n",
    "file_names = ['df.csv', 'df1.csv', 'df3.csv',\n",
    "              'df4.csv']\n",
    "\n",
    "for df, file_name in zip(dataframes, file_names):\n",
    "    # Add the prefix \"cleaned_\" to the file name\n",
    "    cleaned_file_name = \"cleaned_\" + file_name\n",
    "    # Get the full path of the output file\n",
    "    output_file_path = os.path.join(folder_path, cleaned_file_name)\n",
    "    # Save the cleaned DataFrame to CSV\n",
    "    df.to_csv(output_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
