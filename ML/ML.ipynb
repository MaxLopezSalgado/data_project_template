{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Author: Your Name\n",
    "- First Commit: yyyy-mm-dd                      #folowing ISO  8601 Format\n",
    "- Last Commit: yyyy-mm-dd                       #folowing ISO  8601 Format\n",
    "- Description: This notebook is used to perform EDA on the \"xxxxx\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import max_error\n",
    "from sklearn.metrics import mean_poisson_deviance\n",
    "from sklearn.metrics import mean_gamma_deviance\n",
    "from sklearn.metrics import mean_tweedie_deviance\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this jupyter notebook, meant to perform a basic, but useful structure for a ML Project, we will be performing the next actions:\n",
    "\n",
    "**Importing Data**\n",
    "\n",
    "- Read the \"cleaned_df.csv\" file into a DataFrame named \"cleaned_df\".\n",
    "- Display the head, information, and summary statistics of the \"cleaned_df\" DataFrame.\n",
    "\n",
    "**Exploratory Data Analysis**\n",
    "\n",
    "- Visualize correlations between variables using a heatmap.\n",
    "\n",
    "**Data Preprocessing**\n",
    "\n",
    "- Drop a highly correlated column from the DataFrame.\n",
    "\n",
    "**Data Distribution**\n",
    "\n",
    "- Plot histograms to visualize the distribution of the data.\n",
    "\n",
    "**Linearity Analysis**\n",
    "\n",
    "- Create scatter plots to analyze linearity between variables.\n",
    "\n",
    "**Encoding Categorical Variables**\n",
    "\n",
    "- Encode categorical variables using one-hot encoding and create new columns for each category.\n",
    "\n",
    "**Splitting Data**\n",
    "\n",
    "- Split the data into train and test sets.\n",
    "\n",
    "**Scaling Data**\n",
    "\n",
    "- Scale the data using Min-Max scaling.\n",
    "\n",
    "**Model Training and Prediction**\n",
    "\n",
    "- Train a linear regression model on the scaled training data.\n",
    "- Make predictions on the scaled validation data.\n",
    "\n",
    "**Model Evaluation**\n",
    "\n",
    "- Calculate and display evaluation metrics:\n",
    "  - R2 score\n",
    "  - Mean Squared Error\n",
    "  - Mean Absolute Error\n",
    "  - Root Mean Squared Error\n",
    "  - Explained Variance Score\n",
    "\n",
    "**Residual Analysis**\n",
    "\n",
    "- Plot the residuals between the true and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "cleaned_df = pd.read_csv('../cleaned_df.csv')\n",
    "\n",
    "# Explore data\n",
    "display(cleaned_df.head())\n",
    "display(cleaned_df.info())\n",
    "display(cleaned_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for correlations between the variables in a heatmap\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(cleaned_df.corr(), annot=True, cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns that are highly correlated with each other\n",
    "cleaned_df.drop('temp', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look of the distribution of the data\n",
    "cleaned_df.hist(figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for linearity between the variables\n",
    "sns.pairplot(cleaned_df, x_vars=['column1', 'column2', 'column3', 'column4'], y_vars=['column1', 'column2', 'column3', 'column4'], diag_kind='kde')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding cathegorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use get_dummies for encoding categorical variables with unique column names\n",
    "new_season = pd.get_dummies(cleaned_df['month'], prefix='month', drop_first=False)\n",
    "new_weather = pd.get_dummies(cleaned_df['weather'], prefix='weather', drop_first=False)\n",
    "new_year = pd.get_dummies(cleaned_df['year'], prefix='year', drop_first=False)\n",
    "\n",
    "# Drop the old columns\n",
    "cleaned_df.drop(['month', 'weather', 'year'], axis=1, inplace=True)\n",
    "\n",
    "# Concatenate the encoded columns with the original dataframe\n",
    "cleaned_df_encoded = pd.concat([cleaned_df, new_season, new_weather, new_year], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weekday is also a categorical variable. \n",
    "# We will encode them with LabelEncoder and OneHotEncoder\n",
    "# LabelEncoder\n",
    "le = LabelEncoder()\n",
    "cleaned_df_encoded['weekday'] = le.fit_transform(cleaned_df_encoded['weekday'])\n",
    "\n",
    "ohe_weekday = OneHotEncoder()\n",
    "weekday_encoded = ohe_weekday.fit_transform(cleaned_df_encoded['weekday'].values.reshape(-1, 1)).toarray()\n",
    "weekday_df = pd.DataFrame(weekday_encoded, columns=[\"weekday_\" + str(int(i)) for i in range(weekday_encoded.shape[1])])\n",
    "\n",
    "cleaned_df_encoded = pd.concat([cleaned_df_encoded, weekday_df], axis=1)\n",
    "cleaned_df_encoded.drop(['weekday'], axis=1, inplace=True)\n",
    "\n",
    "cleaned_df_encoded.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, validation, and test sets\n",
    "train_df = cleaned_df_encoded[cleaned_df_encoded['year_0'] == 0]\n",
    "test_df = cleaned_df_encoded[cleaned_df_encoded['year_0'] == 1]\n",
    "\n",
    "# Drop the year columns\n",
    "train_df.drop(['year_0', 'year_1'], axis=1, inplace=True)\n",
    "test_df.drop(['year_0', 'year_1'], axis=1, inplace=True)\n",
    "\n",
    "# Split the train data into train and validation sets\n",
    "X_train = train_df.drop('count', axis=1)\n",
    "y_train = train_df['count']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the test data into test and validation sets\n",
    "X_test = test_df.drop('count', axis=1)\n",
    "y_test = test_df['count']\n",
    "\n",
    "X_test, X_val_test, y_test, y_val_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring all the values to a uniform range\n",
    "# Remember that the scaling is applied because the Gradient Descent method that we use to minimize our underlying cost function, converges much faster with scaling than without it.\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and fit the model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_val_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that will calculate the metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    print('R2 score: {}'.format(r2_score(y_true, y_pred)))\n",
    "    print('Mean Squared Error: {}'.format(mean_squared_error(y_true, y_pred)))\n",
    "    print('Mean Absolute Error: {}'.format(mean_absolute_error(y_true, y_pred)))\n",
    "    print('Root Mean Squared Error: {}'.format(np.sqrt(mean_squared_error(y_true, y_pred))))\n",
    "    print('Explained Variance Score: {}'.format(explained_variance_score(y_true, y_pred)))\n",
    "\n",
    "# calculate the metrics\n",
    "calculate_metrics(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that will plot the residuals\n",
    "def plot_residuals(y_true, y_pred):\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.scatter(y_true, residuals)\n",
    "    plt.title('Residual plot')\n",
    "    plt.xlabel('y_true')\n",
    "    plt.ylabel('residuals')\n",
    "    plt.show()\n",
    "\n",
    "# call the function to plot the residuals\n",
    "plot_residuals(y_val, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In addition to the metrics we have already calculated (R2 score, Mean Squared Error, Mean Absolute Error, Root Mean Squared Error, and Explained Variance Score), there are several other metrics that we can analyze to evaluate the performance of our machine learning model. Here are some commonly used metrics:\n",
    "\n",
    "- Accuracy: Accuracy measures the overall correctness of your model's predictions. It is the ratio of the number of correct predictions to the total number of predictions.\n",
    "\n",
    "- Precision: Precision is the proportion of true positive predictions out of all positive predictions. It measures the accuracy of positive predictions.\n",
    "\n",
    "- Recall: Recall is the proportion of true positive predictions out of all actual positive instances. It measures the ability of the model to correctly identify positive instances.\n",
    "\n",
    "- F1 Score: The F1 score is the harmonic mean of precision and recall. It provides a single metric that balances both precision and recall.\n",
    "\n",
    "- ROC AUC Score: ROC (Receiver Operating Characteristic) AUC (Area Under the Curve) score is a performance metric for binary classification models. It measures the model's ability to discriminate between positive and negative classes.\n",
    "\n",
    "- Confusion Matrix: A confusion matrix is a table that shows the counts of true positive, true negative, false positive, and false negative predictions. It provides a detailed breakdown of the model's performance across different classes.\n",
    "\n",
    "- Classification Report: A classification report provides a summary of precision, recall, F1 score, and support for each class in a multi-class classification problem.\n",
    "\n",
    "These metrics can provide additional insights into the performance of our machine learning model, especially in classification tasks. You can calculate these metrics using appropriate functions from libraries such as scikit-learn or other specialized evaluation libraries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
